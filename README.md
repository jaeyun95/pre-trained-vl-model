## Pretrained model summary   
* ### [pretrained language model](#pretrained-language-model)   
* ### [pretrained vision model](pretrained-vision-model)   
* ### [pretrained vision and language model](pretrained-vision-and-language-model)   
---
## Pretrained language model   
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[[paper]](https://arxiv.org/pdf/1810.04805.pdf)[[code(tensorflow)]](https://github.com/google-research/bert)[[code(pytorch)]](https://github.com/codertimo/BERT-pytorch)   
* ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS[[paper]](https://arxiv.org/pdf/1909.11942.pdf)[[code(tensorflow)]](https://github.com/google-research/albert)[[code(pytorch)]](https://github.com/graykode/ALBERT-Pytorch)   
* RoBERTa: A Robustly Optimized BERT Pretraining Approach[[paper]](https://arxiv.org/pdf/1907.11692.pdf)[[code]](https://github.com/pytorch/fairseq/tree/master/examples/roberta)   

---
## Pretrained vision model   

---
## Pretrained vision and language model   

