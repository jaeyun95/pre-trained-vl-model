## Pretrained model summary   
* ### [pretrained language model](#pretrained-language-model)   
* ### [pretrained image model](#pretrained-image-model)   
* ### [pretrained video model](#pretrained-video-model)   
* ### [pretrained image and language model](#pretrained-image-and-language-model)   
* ### [pretrained video and language model](#pretrained-video-and-language-model)   
* ### [pretrained knowledge and language model](#pretrained-knowledge-and-language-model)   
* ### [pretrained vision and language and knowledge model](#pretrained-vision-and-language-and-knowledge-model)  
---
## pretrained language model   
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|Improving Language Understanding by Generative Pre-Training|[[paper]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)|[[code(pytorch)]](https://github.com/huggingface/transformers)|
|ELMo : Deep contextualized word representations|[[paper]](https://arxiv.org/pdf/1802.05365.pdf)|[[code(tensorflow)]](https://github.com/yuanxiaosc/ELMo)|
|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|[[paper]](https://arxiv.org/pdf/1810.04805.pdf)|[[code(tensorflow)]](https://github.com/google-research/bert)[[code(pytorch)]](https://github.com/codertimo/BERT-pytorch)|
|ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS|[[paper]](https://arxiv.org/pdf/1909.11942.pdf)|[[code(tensorflow)]](https://github.com/google-research/albert)[[code(pytorch)]](https://github.com/graykode/ALBERT-Pytorch)|
|RoBERTa: A Robustly Optimized BERT Pretraining Approach|[[paper]](https://arxiv.org/pdf/1907.11692.pdf)|[[code[pytorch]]](https://github.com/pytorch/fairseq/tree/master/examples/roberta)|
|Language Models are Unsupervised Multitask Learners|[[paper]](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)|[[code(tensorflow)]](https://github.com/openai/gpt-2)|
|Language Models are Few-Shot Learners|[[paper]](https://arxiv.org/pdf/2005.14165.pdf)|[[code]](https://github.com/openai/gpt-3)|
|XLNet: Generalized Autoregressive Pretraining for Language Understanding|[[paper]](https://arxiv.org/pdf/1906.08237.pdf)|[[code(tensorflow)]](https://github.com/zihangdai/xlnet)|
---
## pretrained image model   
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|Identity Mappings in Deep Residual Networks|[[paper]](https://arxiv.org/pdf/1603.05027.pdf)|[[code]](https://github.com/KaimingHe/resnet-1k-layers)|
|Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks|[[paper]](https://arxiv.org/pdf/1506.01497.pdf)|[[code(pytorch)]](https://github.com/longcw/faster_rcnn_pytorch)|
|Mask R-CNN|[[paper]](https://arxiv.org/pdf/1703.06870.pdf)|[[code(tensorflow)]](https://github.com/matterport/Mask_RCNN)[[code(pytorch)]](https://github.com/facebookresearch/maskrcnn-benchmark)|
|You Only Look Once: Unified, Real-Time Object Detection|[[paper]](https://arxiv.org/pdf/1506.02640.pdf)|[[code(tensorflow)]](https://github.com/gliese581gg/YOLO_tensorflow)|
|YOLOv3: An Incremental Improvement|[[paper]](https://pjreddie.com/media/files/papers/YOLOv3.pdf)|[[code(tensorflow)]](https://github.com/YunYang1994/tensorflow-yolov3)[[code(pytorch)]](https://github.com/ultralytics/yolov3)|   
|YOLOv4: Optimal Speed and Accuracy of Object Detection|[[paper]](https://arxiv.org/pdf/2004.10934v1.pdf)|[[code(tensorflow)]](https://github.com/hunglc007/tensorflow-yolov4-tflite)|  
|YOLOv5|[paper]|[[code(pytorch)]](https://github.com/ultralytics/yolov5)|  
|Image Transformer|[[paper]](https://arxiv.org/pdf/1802.05751.pdf)|[[code(pytorch)]](https://github.com/sahajgarg/image_transformer)|  

---   
## pretrained video model   
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|Looking Fast and Slow: Memory-Guided Mobile Video Object Detection|[[paper]](https://arxiv.org/pdf/1903.10172v1.pdf)|[[code(tensorflow)]](https://github.com/tensorflow/models)|
|Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection|[[paper]](https://arxiv.org/pdf/1912.03538v3.pdf)|[[code(tensorflow)]](https://github.com/tensorflow/models)|
|Optimizing Video Object Detection via a Scale-Time Lattice|[[paper]](https://arxiv.org/pdf/1804.05472v1.pdf)|[[code(pytorch)]](https://github.com/guanfuchen/video_obj)|
|Mobile Video Object Detection with Temporally-Aware Feature Maps|[[paper]](https://arxiv.org/pdf/1711.06368v2.pdf)|[[code(pytorch)]](https://github.com/thstkdgus35/EDSR-PyTorch)|
|X3D: Expanding Architectures for Efficient Video Recognition|[[paper]](https://arxiv.org/pdf/2004.04730v1.pdf)|[[code(pytorch)]](https://github.com/facebookresearch/SlowFast)|
|SibNet: Sibling Convolutional Encoder for Video Captioning|[[paper]](https://dl.acm.org/doi/pdf/10.1145/3240508.3240667?casa_token=XKKmcvVdwpMAAAAA:AcrWSNb91VmdMipe-6E_AD3Hm2-X9jVU4Zh9-zieWEKItVd6I7pIL0F9n15wjqwX_yl4TrqNcc4Phg)|[code]|
|SAM: Modeling Scene, Object and Action with Semantics Attention Modules for Video Recognition|[[paper]](https://ieeexplore.ieee.org/abstract/document/9316975)|[code]|
|Bottleneck Transformers for Visual Recognition|[[paper]](https://arxiv.org/pdf/2101.11605.pdf)|[[code(pytorch)]](https://github.com/leaderj1001/BottleneckTransformers)|

---   
## pretrained image and language model   
### summary table   
![image](summary.jpg)   

### papaer and code
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks|[[paper]](https://arxiv.org/pdf/1908.02265.pdf)|[[code(pytorch)]](https://github.com/jiasenlu/vilbert_beta)|
|12-in-1: Multi-Task Vision and Language Representation Learning|[[paper]](https://arxiv.org/pdf/1912.02315.pdf)|[[code(pytorch)]](https://github.com/facebookresearch/vilbert-multi-task)|
|LXMERT: Learning Cross-Modality Encoder Representations from Transformers|[[paper]](https://arxiv.org/pdf/1908.07490.pdf)|[[code(pytorch)]](https://github.com/airsplay/lxmert)|
|VISUALBERT: A SIMPLE AND PERFORMANT BASELINE FOR VISION AND LANGUAGE|[[paper]](https://arxiv.org/pdf/1908.03557.pdf)|[[code(pytorch)]](https://github.com/uclanlp/visualbert)|
|VL-BERT: Pre-training of Generic Visual-Linguistic Representations|[[paper]](https://arxiv.org/pdf/1908.08530.pdf)|[[code(pytorch)]](https://github.com/jackroos/VL-BERT)|
|UNITER: LEARNING UNIVERSAL IMAGE-TEXT REPRESENTATIONS|[[paper]](https://arxiv.org/pdf/1909.11740.pdf)|[[code(pytorch)]](https://github.com/ChenRocks/UNITER)|
|Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training|[[paper]](https://arxiv.org/pdf/1908.06066.pdf)|[[code(pytorch)]](https://github.com/microsoft/Unicoder)|
|Large-Scale Adversarial Training for Vision-and-Language Representation Learning|[[paper]](https://arxiv.org/pdf/2006.06195.pdf)|[[code(pytorch)]](https://github.com/zhegan27/VILLA)|
|Fusion of Detected Objects in Text for Visual Question Answering|[[paper]](https://arxiv.org/pdf/1908.05054.pdf)|[[code(tensorflow)]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2)|
|ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph|[[paper]](https://arxiv.org/pdf/2006.16934.pdf)|[[code]](https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-vil)|
|X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers|[[paper]](https://arxiv.org/pdf/2009.11278.pdf)|[[code]](https://prior.allenai.org/projects/x-lxmert)|
|Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers|[[paper]](https://arxiv.org/pdf/2004.00849.pdf)|[code]|
|M6-v0: Vision-and-Language Interaction for Multi-modal Pretraining|[[paper]](https://arxiv.org/pdf/2003.13198.pdf)|[code]|
|Unified Vision-Language Pre-Training for Image Captioning and VQA|[[paper]](https://arxiv.org/pdf/1909.11059.pdf)|[code]|
|Multimodal Pretraining Unmasked:Unifying the Vision and Language BERTs|[[paper]](https://arxiv.org/pdf/2011.15124.pdf)|[[code]](https://github.com/e-bug/mpre-unmasked)|
|VinVL: Making Visual Representations Matter in Vision-Language Models|[[paper]](https://arxiv.org/pdf/2101.00529.pdf)|[code]|
|Seeing past words: Testing the cross-modal capabilities of pretrained V&L models|[[paper]](https://arxiv.org/pdf/2012.12352.pdf)|[code]|
|Inferring spatial relations from textual descriptions of images|[[paper]](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320321X00027/1-s2.0-S0031320321000340/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEP7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIAFrpMQhR5seYXLGAigs9%2Bh8RmIsXEFE%2BUiUJh5SpcYUAiEAnZJyTgmsngv7L5ulh%2BdwsG%2BpC9nVCqGYzcUhEdP2snsqtAMIFxADGgwwNTkwMDM1NDY4NjUiDLHiSODGY3jKVj5rJCqRA1Ced2tjmzKdofdI%2BOMNSSWyc5vwzGQA5wcS%2BRMJ3EtTZCC0Wrf4dDZzB6c0f%2Fyn65anudRLZCwRNVpSLbYUt6NFMmU3uF5tqKeL%2FOSNpLGqNVcz9RiIgbUNSSpv0SbRt9fRWyVdzfOSVzcBm4%2FSe1W5MRrT1jjNsPTv%2FAL2Jh99daWJvSQ%2BiVNas83%2Bxvs%2BjT90y6%2BibncFVF4tyQ3b8e9yXNuuwLfcpqoVIhfFuoRBKHpSGUuTWqi1YGSmRrxa%2FvCW2iVNjwIc%2FGmGjXsJc5r164jTeAxs9HNezHv4npNhkp6essXP4KlLm2J%2FgPHflemTtvHQKY4jnJsqvQSvneHwcHO4KPMxawpfQoVkHZvEpQlrlyZMS3MVpHXqDZ%2FszArUOiQzQupuTWyIqRUm5kq%2FteP7vI76pTZaEK8xlSEU7CXbgdazZ9ODA%2Btzcy047GUjDNaX4whpOw1WvITfoAX9yx2Q8nXTlwbeT0nwGYANmV26GJDi7ERWF1XCeEXl3abogiIusqCMDvjaK0DtwOzvMNyr2YEGOusBFeybcG6Izx40ogGtkALRpZYVMu1PyYA0PB0AFS9Y1n4Xy%2FMa%2BpaSNzTHvLx2To5wpZWjP%2Bf%2Ft3iRoBJA92uGMKVK4HXkcBnBPPXktJu2Q9ySESWus2pfvApW3ptAijiBUQkW5wyL5o4%2F2W9BFE%2Bpzp2m%2FiH6siNPRssofVHOF%2FWeZ0lHw62NZCVFzWQpCErPBQfXCOPOdqpyNnw8ZXM5dSN2v3yTGrfQVg5hCnFAUmMPOxL1FQ%2B1NqwSQLH8aWILzkQuSCAtkHWpntMCh5DZiDaM2DP5JWlIvUClvaRIO6EorY%2BZ%2Fxol69W%2BOQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210224T142215Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3Y7NWSNW%2F20210224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=2daf656306e9b6b750f0e015841b596a0ea84121b2c883f484aedb374018205f&hash=c2d1cd118302f94013900b5f2d12298b910ef5ae4cf4744e28240eb9bb68d358&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320321000340&tid=spdf-b7a0f012-f071-416c-8827-3412944323b0&sid=b7c0003155bce540411997d23097894bc9a9gxrqa&type=client)|[code]|
|DMRFNet: Deep Multimodal Reasoning and Fusion for Visual Question Answering and explanation generation|[[paper]](https://pdf.sciencedirectassets.com/272144/1-s2.0-S1566253521X00036/1-s2.0-S1566253521000208/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEC0aCXVzLWVhc3QtMSJHMEUCIQCtJ3G%2BQuOAM0V5L45uafdA432CPULxorqX9YKW9azn2AIgO3y3kKmapYsjmz28OU5kkaF6etW%2FtfZcR2FazhGqODEqtAMIRhADGgwwNTkwMDM1NDY4NjUiDPiFMg7TG10fqYhRqiqRA0Yhw1OXldLZVeiHaTihMsbxUl%2F5rXTfJWh0oYqUy5D4yur78bgoYlr4zGDsJ2J15h0DCJE0uKLbEggd2qldJKbtFK3hhLG1%2BwPAvnhM0x8vq%2FpqF858%2BHuK2OUk3FO6EEUn0KrZVYxlNMDo0HISQcQmLaXQ8i3hgOpmxlaDMMmOtphi45Trtwf86eNH9Ci3CyW0Ridak8jMXeWxnZCuzT8qGW%2FE73oUf8X%2BM6SGfUYEm5QAgpoTXfMm%2FZjld1vERa5E1S2c4YRnd%2FWJlUAT9vmV9TZ6Nn8BYpbz%2Buo71KZzxvW1XPiND9CcmUCvT0IPTz8kQ2w%2F0dRw%2Fy%2BgVHOBChXbowUZ1BDNNRF6zVdakCvN85whj%2Fbdymq64wFWDtTsmafvs8CaXDBtE86dfPgmLOvar%2BbWKIOaFQJoQhtz8pozjo%2F4FfkxnA%2FtxXbYbBwU5S9IK5qmvx4O9Leg0I3Fpd6b19Vw9e%2Fr40vrUq96AIXhpdRorYtuzJ1Qi0WTus2E%2BUDsQ1JDXCpApVL6CZ9ek1NbMMDX44EGOusBplT9ln64HfsPpc1IGOlPaKjAyxPNuWctrU8Zny%2Fz55XdkvDY2qSLNJ6UP%2BU9MYEm1UpOWZCJXGXfNyY3DU3czo4NA7lbKDSlxnV7NxCynvWo3oVUNp1vpmrHUqrJGikXGlZbsl8Dayr6HjsJ5DZexmSaQ6QXETs4lIEaETFp74OdZ0CGvMQtws557p5OhPuzjrJXD11utBbMYyI0bf57by727pmOszERNM4RcMx5TaSAXmEyakzFoh4AYb%2F9guHGa7TG13%2FxKvXzYt%2FLTauKDMl8vt%2BUubBPRDBGACGzQMjCcj3G9ULKuF9V8A%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210226T134630Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYSSC7D777%2F20210226%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0d230c32831c2f9708cd2afd96367e861d937f87dd22d10a1bb82d63e0334b95&hash=b77b53d6ac0e6e3615b3fb187f95eda62c0d8da68475971fdc1bf2eb8cacfca2&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1566253521000208&tid=spdf-bb7adf16-4c4f-4096-9d8d-a45d77b74549&sid=a6fddab4825360470339cbc978b5a71e5d1dgxrqa&type=client)|[code]|

---
## pretrained video and language model   
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|VideoBERT: A Joint Model for Video and Language Representation Learning|[[paper]](https://arxiv.org/pdf/1904.01766.pdf)|[code]|
|UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation|[[paper]](https://arxiv.org/pdf/2002.06353.pdf)|[code]|
|Multi-modal Circulant Fusion for Video-to-Language and Backward|[[paper]](https://www.ijcai.org/Proceedings/2018/0143.pdf)|[code]|
|Video-Grounded Dialogues with Pretrained Generation Language Models|[[paper]](https://arxiv.org/pdf/2006.15319.pdf)|[code]|
|Deep Extreme Cut: From Extreme Points to Object Segmentation|[[paper]](https://arxiv.org/pdf/1711.09081)|[[code(pytorch)]](https://github.com/scaelles/DEXTR-PyTorch)|
|Integrating Multimodal Information in Large Pretrained Transformers|[[paper]](https://arxiv.org/pdf/1908.05787.pdf)|[[code(pytorch)]](https://github.com/dongjunn/MBERT)|
|Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text|[[paper]](https://arxiv.org/pdf/1604.01729.pdf)|[[code(caffe)]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/language_fusion)|
|PARAMETER EFFICIENT MULTIMODAL TRANSFORMERS FOR VIDEO REPRESENTATION LEARNING|[[paper]](https://arxiv.org/pdf/2012.04124.pdf)|[code]|
|LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval|[[paper]](https://openaccess.thecvf.com/content/WACV2021/papers/Tan_LoGAN_Latent_Graph_Co-Attention_Network_for_Weakly-Supervised_Video_Moment_Retrieval_WACV_2021_paper.pdf)|[code]|
|VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs|[[paper]](https://arxiv.org/pdf/2101.12059.pdf)|[code]|
|HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training|[[paper]](https://arxiv.org/pdf/2005.00200.pdf)|[[code(pytorch)]](https://github.com/linjieli222/HERO)|
|Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling|[[paper]](https://arxiv.org/pdf/2102.06183.pdf)|[[code(pytorch)]](https://github.com/jayleicn/ClipBERT)|

---
## pretrained knowledge and language model   
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|Knowledge Enhanced Contextual Word Representations|[[paper]](https://arxiv.org/pdf/1909.04164.pdf)|[[code(pytorch)]](https://github.com/allenai/kb)|
|Why Do Masked Neural Language Models Still Need Commonsense Repositories to Handle Semantic Variations in Question Answering? |[[paper]](https://arxiv.org/pdf/1911.03024.pdf)|[code]|
|SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge|[[paper]](https://www.aclweb.org/anthology/2020.emnlp-main.567.pdf)|[[code]](https://github.com/thu-coai/SentiLARE/issues)|
|Acquiring Knowledge from Pre-trained Model to Neural Machine Translation|[[paper]](https://arxiv.org/pdf/1912.01774.pdf)|[[code]](https://github.com/wengrx/APT-NMT)|
|Knowledge-Aware Language Model Pretraining|[[paper]](https://arxiv.org/pdf/2007.00655.pdf)|[code]|
|Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model|[[paper]](https://arxiv.org/pdf/1912.09637.pdf)|[code]|

---
## pretrained vision and language and knowledge model   
|**title**|**paper link**|**code link**|
|---|:---:|:---:|
|Reasoning over Vision and Language:Exploring the Benefits of Supplemental Knowledge|[[paper]](https://arxiv.org/pdf/2101.06013.pdf)|[code]|
